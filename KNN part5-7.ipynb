{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9298245614035088\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Train the model\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict the testing set\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9291680588038758\n",
      "Recall: 0.9207337045528987\n",
      "F1-score: 0.9246031746031746\n",
      "Confusion Matrix:\n",
      " [[38  5]\n",
      " [ 3 68]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Practices:\n",
    "##### Normalize Features: Since KNN relies on distance metrics, it's essential to normalize or standardize the features to have a similar scale. This prevents features with larger scales from dominating the distance calculations.\n",
    "##### Choose Appropriate Distance Metric: Depending on the nature of your data, choose the appropriate distance metric. Euclidean distance is commonly used, but for categorical data or sparse data, other metrics like Hamming distance or cosine similarity might be more suitable.\n",
    "##### Optimize K: The choice of K significantly impacts the model's performance. Selecting an appropriate value of K through techniques like cross-validation or grid search is crucial. Too small K can lead to overfitting, while too large K can lead to underfitting.\n",
    "##### Data Quality: Ensure that your dataset is clean and free from outliers, as KNN is sensitive to noisy data.\n",
    "##### Feature Selection: Selecting relevant features can improve the performance of KNN. Use techniques like feature importance, PCA (Principal Component Analysis), or domain knowledge to choose the most informative features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common Pitfalls:\n",
    "##### Curse of Dimensionality: KNN suffers from the curse of dimensionality, where the feature space becomes increasingly sparse in high-dimensional spaces. This can lead to increased computation and decreased performance. Dimensionality reduction techniques like PCA can help mitigate this issue.\n",
    "##### Imbalanced Data: KNN tends to favor the majority class in imbalanced datasets. It's essential to balance the dataset or use techniques like oversampling, undersampling, or using weighted KNN to handle class imbalance.\n",
    "##### Computational Complexity: KNN's prediction time grows linearly with the size of the training dataset. For large datasets, this can become computationally expensive. Approximate nearest neighbor algorithms or tree-based approaches like KD-trees or ball trees can be used to speed up computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization Techniques:\n",
    "##### Ball Trees and KD-Trees: These data structures can be used to efficiently store and query the data points, reducing the time complexity of the nearest neighbor search.\n",
    "##### Neighborhood Search Algorithms: Techniques like locality-sensitive hashing (LSH) can be used to approximate nearest neighbors more efficiently, especially in high-dimensional spaces.\n",
    "##### Parallelization: KNN computations can be parallelized across multiple processors or nodes to speed up the process, especially for large datasets.\n",
    "##### Algorithmic Variants: Variants of KNN like K-D Trees, Ball Trees, or Approximate Nearest Neighbors (ANN) can be used to optimize performance for specific scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Points:\n",
    "##### KNN Algorithm: KNN is a simple yet powerful supervised machine learning algorithm used for classification and regression tasks.\n",
    "##### Operation: During training, KNN memorizes feature vectors and their corresponding labels. In prediction, it calculates distances between the new data point and every other point in the dataset and selects the top K nearest neighbors based on a chosen distance metric.\n",
    "##### Parameters: The choice of parameter K significantly affects the model's performance. It's essential to select an appropriate value of K through techniques like cross-validation.\n",
    "##### Performance Metrics: Common performance evaluation metrics for KNN include accuracy, precision, recall, F1-score, and confusion matrix.\n",
    "##### Best Practices: Normalize features, choose an appropriate distance metric, optimize K, ensure data quality, and select relevant features to improve the algorithm's performance.\n",
    "##### Common Pitfalls: KNN suffers from the curse of dimensionality, imbalanced data, and computational complexity. Careful consideration and preprocessing of data are essential to address these issues.\n",
    "##### Optimization Techniques: Techniques like using KD-trees or ball trees, neighborhood search algorithms, parallelization, and algorithmic variants can optimize the performance of KNN, especially for large datasets and high-dimensional spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Future Directions or Areas for Improvement:\n",
    "##### Scalability: Enhancing the scalability of KNN for large datasets and high-dimensional spaces is an ongoing area of research. Developing efficient algorithms and parallelization techniques can address this challenge.\n",
    "##### Handling Noise: KNN is sensitive to noisy data. Exploring robust distance metrics or incorporating noise-handling mechanisms can improve its robustness.\n",
    "##### Adaptability: Developing adaptive KNN models that dynamically adjust K or distance metrics based on the characteristics of the data could improve its generalization ability across diverse datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Thoughts:\n",
    "KNN is a versatile algorithm with various applications in classification and regression tasks. While it's straightforward to implement and understand, careful parameter tuning and preprocessing are essential for its effective use. By following best practices, avoiding common pitfalls, and exploring optimization techniques, KNN can be a valuable tool in a data scientist's toolkit. However, it's crucial to keep in mind its limitations and continuously explore ways to enhance its performance and applicability in real-world scenarios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
