{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9298245614035088\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Train the model\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict the testing set\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9291680588038758\n",
      "Recall: 0.9207337045528987\n",
      "F1-score: 0.9246031746031746\n",
      "Confusion Matrix:\n",
      " [[38  5]\n",
      " [ 3 68]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips and Tricks\n",
    "## 1.Best Practices\n",
    "## 2.Common Pitfalls\n",
    "## 3.Optimization techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Practices:\n",
    "* ## Normalize Features: Since KNN relies on distance metrics, it's important to normalize or standardize the features to have a similar scale. This prevents features with larger scales from dominating the distance calculations.\n",
    "* ## Choose Appropriate Distance Metric: Depending on the nature of the data, choose the appropriate distance metric. Euclidean distance is commonly used, but for categorical or sparse data, other metrics such as Hamming distance or cosine similarity might be more suitable.\n",
    "* ## Optimize K: The choice of K can have a significant impact on the performance of the model. It is critical to select the appropriate value of K through techniques such as cross-validation or grid searching; too small a value of K can lead to overfitting, while too large a value can lead to underfitting.\n",
    "* ## Data Quality: Make sure the dataset is clean and free of anomalies, as KNN is sensitive to noisy data.\n",
    "* ## Feature Selection: Selecting relevant features can improve the performance of KNN. Use techniques such as feature importance, PCA (Principal Component Analysis) or domain knowledge to select the most informative features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Pitfalls:\n",
    "* ## Curse of Dimensionality: KNN is affected by the curse of dimensionality, where the feature space becomes increasingly sparse in high dimensional spaces. This leads to increased computation and performance degradation. dimensionality reduction techniques such as PCA can help alleviate this problem.\n",
    "* ## Imbalanced Data: In unbalanced datasets, KNNs tend to favour the majority class. Therefore, it is necessary to balance the dataset or use techniques such as oversampling, undersampling, or using weighted KNNs to deal with class imbalance.\n",
    "* ## Computational Complexity: The prediction time of KNN grows linearly with the size of the training dataset. For large datasets, the computational cost can become very expensive. The computation can be speeded up by using approximate nearest neighbour algorithms or tree-based methods such as KD trees or ball trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Techniques:\n",
    "* ## Ball Trees and KD-Trees: These data structures can be used to efficiently store and query data points, thus reducing the time complexity of the nearest neighbour search.\n",
    "* ## Neighborhood Search Algorithms: Techniques such as location-sensitive hashing (LSH) can be used to approximate nearest neighbours more efficiently, especially in high-dimensional spaces.\n",
    "* ## Parallelization: KNN computations can be performed in parallel on multiple processors or nodes to speed up computation, especially when working with large datasets.\n",
    "* ## Algorithmic Variants: Variants of KNN, such as K-D trees, ball trees or Approximate Nearest Neighbours (ANN), can be used to optimise performance in specific scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Points:\n",
    "* ## KNN Algorithm: KNN is a simple yet powerful supervised machine learning algorithm for classification and regression tasks.\n",
    "* ## Operation: During training, the KNN remembers the feature vectors and their corresponding labels. At prediction time, it calculates the distance between the new data point and every other point in the dataset and selects the top K nearest neighbours based on the selected distance metric.\n",
    "* ## Parameters: The choice of parameter K can greatly affect the performance of the model. It is crucial to select the appropriate value of K through techniques such as cross-validation.\n",
    "* ## Performance Metrics: Common performance evaluation metrics for KNN include accuracy, precision, recall, F1-score and confusion matrix.\n",
    "* ## Best Practices: The features are normalised, a suitable distance metric is chosen, K is optimised to ensure data quality and relevant features are selected to improve the performance of the algorithm.\n",
    "* ## Common Pitfalls: KNN suffers from the curse of dimensionality, data imbalance and computational complexity. To solve these problems, the data must be carefully considered and preprocessed.\n",
    "* ## Optimization Techniques: The use of techniques such as KD or ball trees, neighbourhood search algorithms, parallelisation and algorithmic variants can optimise the performance of KNNs, especially in large datasets and high-dimensional spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Directions or Areas for Improvement:\n",
    "* ## Scalability: Improving the scalability of KNNs in large datasets and high-dimensional spaces is an ongoing area of research. The development of efficient algorithms and parallelisation techniques can address this challenge.\n",
    "* ## Handling Noise: KNN is sensitive to noisy data. Exploring robust distance metrics or incorporating noise handling mechanisms can improve its robustness.\n",
    "* ## Adaptability: The development of adaptive KNN models that dynamically adjust the K or distance metric based on data features can improve their generalisation ability across different datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Thoughts.:\n",
    "### KNN is a versatile algorithm with multiple applications in classification and regression tasks. While it is easy to implement and understand, careful parameter tuning and preprocessing are essential for its effective use. By following best practices, avoiding common pitfalls, and exploring optimisation techniques, KNN can be an important tool in a data scientist's toolkit. However, it is critical to keep its limitations in mind and constantly explore ways to improve its performance and applicability in real-world scenarios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
