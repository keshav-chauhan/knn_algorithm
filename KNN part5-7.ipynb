{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9298245614035088\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Train the model\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict the testing set\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9291680588038758\n",
      "Recall: 0.9207337045528987\n",
      "F1-score: 0.9246031746031746\n",
      "Confusion Matrix:\n",
      " [[38  5]\n",
      " [ 3 68]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips and Tricks\n",
    "## 1.Best Practices\n",
    "## 2.Common Pitfalls\n",
    "## 3.Optimization techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Practices:\n",
    "* ## Normalize Features: Since KNN relies on distance metrics, it's important to normalize or standardize the features to have a similar scale.\n",
    "* ## Choose Appropriate Distance Metric: Depending on the nature of the data, the appropriate distance metric is chosen.\n",
    "* ## Optimize K: The choice of K value can have a significant impact on the performance of the model. It is critical to select the appropriate K-value through techniques such as cross-validation or grid search.\n",
    "* ## Data Quality: Make sure the dataset is clean and free of anomalies, as KNN is sensitive to noisy data.\n",
    "* ## Feature Selection: Selecting relevant features can improve the performance of KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Pitfalls:\n",
    "* ## Curse of Dimensionality: In high dimensional spaces, the feature space of KNN becomes increasingly sparse.\n",
    "* ## Imbalanced Data: In unbalanced datasets, KNNs tend to favour most categories.\n",
    "* ## Computational Complexity: The prediction time of KNN grows linearly with the size of the training dataset. For large datasets, the computational cost becomes very expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Techniques:\n",
    "* ## Ball Trees and KD-Trees: These data structures can be used to efficiently store and query data points, thus reducing the time complexity of the nearest neighbour search.\n",
    "* ## Neighborhood Search Algorithms: Techniques such as location-sensitive hashing (LSH) can be used to approximate nearest neighbours more efficiently, especially in high-dimensional spaces.\n",
    "* ## Parallelization: KNN computations can be performed in parallel on multiple processors or nodes to speed up computation, especially when working with large datasets.\n",
    "* ## Algorithmic Variants: Variants of KNN, such as K-D trees, ball trees or Approximate Nearest Neighbours (ANN), can be used to optimise performance in specific scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Points:\n",
    "* ## KNN Algorithm: KNN is a simple yet powerful supervised machine learning algorithm for classification and regression tasks.\n",
    "* ## Operation: During training, the KNN remembers the feature vectors and their corresponding labels. At prediction time, it calculates the distance between the new data point and every other point in the dataset and selects the top K nearest neighbours based on the selected distance metric.\n",
    "* ## Parameters: The choice of parameter K can greatly affect the performance of the model.\n",
    "* ## Performance Metrics: Common performance evaluation metrics for KNN include accuracy, precision, recall, F1-score and confusion matrix.\n",
    "* ## Best Practices: The features are normalised, a suitable distance metric is chosen, K is optimised to ensure data quality and relevant features are selected to improve the performance of the algorithm.\n",
    "* ## Common Pitfalls: KNN suffers from curse of dimensionality, data imbalance and computational complexity.\n",
    "* ## Optimization Techniques: The performance of KNNs can be optimised using techniques such as KD or ball trees, neighbourhood search algorithms, parallelisation and algorithmic variants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Directions or Areas for Improvement:\n",
    "* ## Scalability: The development of efficient algorithms and parallelisation techniques can improve the scalability of KNN in large datasets and high-dimensional spaces.\n",
    "* ## Handling Noise: Exploring robust distance metrics or incorporating noise handling mechanisms can improve KNN robustness.\n",
    "* ## Adaptability: The development of adaptive KNN models that dynamically adjust the K or distance metric based on data features can improve their generalisation ability across different datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Thoughts:\n",
    "* ## KNN is a versatile algorithm with multiple applications in classification and regression tasks.\n",
    "* ## KNN is easy to implement and understand, but careful parameter tuning and preprocessing are essential for its effective use.\n",
    "* ## KNN is an important tool in the data scientist's toolkit.\n",
    "* ## It is important to keep in mind the limitations of KNN and keep exploring ways to improve its performance and applicability in real-world scenarios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
